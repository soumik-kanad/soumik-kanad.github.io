<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://soumik-kanad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://soumik-kanad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-17T23:37:34+00:00</updated><id>https://soumik-kanad.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Claiming something was already discovered in ancient India, a case study.</title><link href="https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws/" rel="alternate" type="text/html" title="Claiming something was already discovered in ancient India, a case study."/><published>2023-12-06T00:00:00+00:00</published><updated>2023-12-06T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws/"><![CDATA[<p>A lot of Indians love to claim that scientific things were already discovered/invented in ancient India. Although, this is not always incorrect (you can find Panini’s name in <a href="https://en.wikipedia.org/wiki/List_of_pioneers_in_computer_science">the list of pioneers in computer science</a> ), most of these claims are unsubstantiated. Recently, I came across a youtube video claiming Netwon’s Laws of Motion were alreay discovered by <a href="https://en.wikipedia.org/wiki/Ka%E1%B9%87%C4%81da_(philosopher)">Kanada</a>, an ancient Indian plilosopher who is known for his atomist approach of matter (Kana). A big problem with such videos is that they don’t do their research very well and just publish anything that resonates with their ideology.</p> <h2 id="tldr">tl;dr</h2> <p>Please properly check your sources before claiming that “it was already discovered in ancient India”. It is possible that it was but please don’t publish it till you check it, otherwise it just becomes misinformation.</p> <h2 id="validity-of-the-video">Validity of the Video</h2> <iframe width="315" height="560" src="https://www.youtube.com/embed/dCMPy5DHoMs?si=vhJdu9lS-1o8UezZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> <p>A short video titled <a href="https://youtube.com/shorts/dCMPy5DHoMs?si=vhJdu9lS-1o8UezZ">Is NEWTON Really GREAT? | This will Shock you</a> was published on Youtube. I got intrigued because I share my nickname with Rishi Kanada, a name I got from my late maternal grandfather. This video cites a 2020 paper named <a href="https://www.ajer.org/papers/Vol-9-issue-7/K09078792.pdf">Origin of Laws of Motion (Newton’s Law): An Introspective Study</a> which was written by a 2<sup>nd</sup> year bachelor’s student without any supervision and was publised in a <a href="https://www.ajer.org">questionable journal</a><d-footnote><a href="https://flakyj.blogspot.com/2018/12/american-journal-of-engineering.html">https://flakyj.blogspot.com/2018/12/american-journal-of-engineering.html</a></d-footnote><d-footnote><a href="https://twitter.com/fake_journals/status/1057638078888591360?lang=en">https://twitter.com/fake_journals/status/1057638078888591360?lang=en</a></d-footnote>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/paper_screenshot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/paper_screenshot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/paper_screenshot-1400.webp"/> <img src="/assets/img/blog/paper_screenshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For a second, even if I were to stop questioning this journal and the credential of the author, and go into the paper itself, it still has problems. The sutras (principles) being talked about in the paper don’t even exist in <a href="https://indianculture.gov.in/rarebooks/vaisesika-sutras-kanada-commentary-sankara-misra-and-extracts-gloss-jayanarayana">The Vaisheshika Sutras of Kanada</a>. Yes, I checked and none of the sutras start with the word “Vegah” (वेगः meaning impetus). After some reverse engineering I found a <a href="https://www.booksfact.com/science/ancient-science/vaishesika-sutras-by-kanada-describe-laws-of-motion-concept-of-atom.html">2013 webpage with identical sutras</a> and a <a href="https://www.quora.com/Did-Newton-take-three-laws-of-motion-from-Ancient-Indian-scriptures">2018 quora question</a> (which probably also picked these up from the former) which have these sutras.</p> <p><b>This means that someone made these up and others are propagating this misinformation to the extent that someone published it in a questionable journal and someone else used that paper to bolster their claim in a video. </b> There is already so much misinformation and “already discovered in ancient India” claims don’t help stuff that were actually discovered in ancient India. People have started considering “already discovered in ancient India” as a joke.</p> <h2 id="were-newtons-laws-of-motion-discovered-by-kanada-first">Were Newton’s Laws of Motion discovered by Kanada first?</h2> <p>In the previous section I have given evidence as to why the sources of the video linked is fabricated. But that doesn’t prove the claim wrong and one might wonder if the claim if actually correct. The answer to that is yes, and no. I did come across an insightful blog by <a href="https://en.wikipedia.org/wiki/Subhash_Kak">Dr. Subhash Kak</a> titled <a href="https://subhashkak.medium.com/ka%E1%B9%87%C4%81da-the-great-physicist-and-sage-of-antiquity-5a2abd79b6f1">“Kaṇāda, Great Physicist and Sage of Antiquity”</a> which does point out to sutras (which actually exist in Vaisheshika Sutras, I checked) which might have the essence of being predecessor ideas to more concrete and explicit versions of Newton’s Laws of Motion. Check out yourself.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_screenshot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_screenshot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_screenshot-1400.webp"/> <img src="/assets/img/blog/blog_screenshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I also found a <a href="https://www.quora.com/Did-Newton-take-three-laws-of-motion-from-Ancient-Indian-scriptures/answer/Abdul-Azarudeen-1?comment_id=182815984&amp;comment_type=2">comment in the quora question’s answers</a> which did point to exact sutras corresponding to each of the laws:</p> <h3 id="1st-law">1st law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/1st_law-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/1st_law-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/1st_law-1400.webp"/> <img src="/assets/img/blog/1st_law.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="2nd-law">2nd law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2nd_law1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2nd_law1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2nd_law1-1400.webp"/> <img src="/assets/img/blog/2nd_law1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2nd_law2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2nd_law2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2nd_law2-1400.webp"/> <img src="/assets/img/blog/2nd_law2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="3rd-law">3rd law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/3rd_law-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/3rd_law-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/3rd_law-1400.webp"/> <img src="/assets/img/blog/3rd_law.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="conclusion">Conclusion</h3> <p>Even though not exactly the same, these do point to the fact the ancient Indian philosophers might have been thinking in the right direction. This is probably the difference between philosophy and science (I will write a blog about this in the future). And I appreciate the efforts of Rishi Kanada for this. <b>It is especially very surprising for me that he nailed the 3rd law before the Common Era (BCE)</b>. Which is very cool!</p> <p>If you are interested here are 2 different translations/interpretations of “The Vaisheshika Sutras of Kanada”:</p> <ol> <li><a href="https://indianculture.gov.in/rarebooks/vaisesika-sutras-kanada-commentary-sankara-misra-and-extracts-gloss-jayanarayana">The Vaisesika Sutras of Kaṇada: With the Commentary of Sankara Misra and Extracts From the Gloss of Jayanarayaṇa </a></li> <li><a href="https://www.omraizada.com/home/scriptures/Vaisheshika%20Sutra-of-Kanada-Debashish-Chakrabarty.pdf">Vaisesika Sutra of Kanada, Translated by Debasish Chakravarty</a></li> </ol>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[A lot of Indians love to claim that scientific things were already discovered/invented in ancient India. Although, this is not always incorrect (you can find Panini’s name in the list of pioneers in computer science ), most of these claims are unsubstantiated. Recently, I came across a youtube video claiming Netwon’s Laws of Motion were alreay discovered by Kanada, an ancient Indian plilosopher who is known for his atomist approach of matter (Kana). A big problem with such videos is that they don’t do their research very well and just publish anything that resonates with their ideology.]]></summary></entry><entry><title type="html">AI vs Jobs?</title><link href="https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements/" rel="alternate" type="text/html" title="AI vs Jobs?"/><published>2023-07-12T00:00:00+00:00</published><updated>2023-07-12T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements/"><![CDATA[<p>Have you ever wondered which kinds of jobs are easier to automate? For example, with the advent of the Industrial Revolution, electro-mechanical machines could automate a lot of repetitive (“non-intelligent”) physical labour. Similarly, computers have automated a lot of computations that were done by hand in the olden days. Now, as we see how powerful AI can be, the next question is which kinds of automation will it aid? There are multiple ways to make this prediction and this blog talks about one such point of view. I would like to emphasise that this might not be the right answer but it is definitely an interesting one.</p> <h2 id="tldr">tl;dr</h2> <p>It took hundreds of times more time for locomotion to evolve than human level intelligence. Is this why it seems that creative jobs are going to be replaced by AI before physical labour jobs as the former is an easier problem to solve?</p> <h2 id="blue-collar-vs-white-collar">Blue collar vs white collar</h2> <p>In a simplified classification of jobs, one could categorise them into three classes - physical labour jobs, mental labour jobs, and creative jobs. There could be other categories or jobs that might not fit these categories but I think these are quite prevalent. Physical labour jobs include jobs like construction jobs, plumbing, electrical jobs, and other physical maintenance jobs while mental labour jobs include other administrative desk jobs which do need a skill set at times but may not need creativity as such like clerical jobs, annotators, programmers, customer service, etc. Finally, creative jobs include jobs like music, art, research, etc. where something new is being created.</p> <p>Scientists in the previous century thought that if we were successful in creating artificial intelligence (AI), it would take jobs in the same order of categories as I list them above<d-footnote>Need citation.</d-footnote>. I think this was thought because people put these categories of jobs in a hierarchy of prestige. This comes from the notion that a lot of people can work in a factory while not many can write poetry or a paper. But as we move closer to the middle of the 21<sup>st</sup> century, it seems that the opposite order may be more probable. Today we see that just with prompts one can create beautiful art, music, or poetry but we are far away from perfectly working autonomous cars or other forms of robotics.</p> <p>Have you ever wondered how we reached this point where more people can do physical labour than creative thinking? One interesting analogy to see why that may be the case lies in the evolution of human intelligence vs animal locomotion. It is noteworthy to see that human intelligence evolved only recently, for example, humans started using tools only 3.5 million years (Pliocene epoch <d-footnote><a href="https://en.wikipedia.org/wiki/Evolution_of_human_intelligence">https://en.wikipedia.org/wiki/Evolution_of_human_intelligence</a></d-footnote>) ago. On the other hand locomotion in animals date back to at least 560 million years <d-footnote><a href="https://www.scientificamerican.com/article/the-rise-of-the-first-animals">https://www.scientificamerican.com/article/the-rise-of-the-first-animals/</a></d-footnote>. This shows how much evolution was required to perfect (or rather just to make functional) our physical movement (with which also comes planning) while only a fraction of that time was invested in evolving human intelligence.</p> <p>This may not be perfect evidence but does <strong>point towards the possibility of creativity being an easier problem to solve than physical labour</strong>. Even though this is not proof of any sort but it is a powerful idea and worth thinking about. Another reason that might have caused creatively intelligent tasks solved before more physically skilled tasks may be because of the abundance of data in the creative world due to the internet compared to the physical labour world. But these are all speculations and only time will tell which was the actual reason.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>I don’t think this is my original thought. I came across this in a video that had a Youtuber called <a href="https://www.youtube.com/@VarunMayya">Varun Mayya</a>. I tried finding it but could find it given the abundance of these short-form videos these days.</p> ]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Have you ever wondered which kinds of jobs are easier to automate? For example, with the advent of the Industrial Revolution, electro-mechanical machines could automate a lot of repetitive (“non-intelligent”) physical labour. Similarly, computers have automated a lot of computations that were done by hand in the olden days. Now, as we see how powerful AI can be, the next question is which kinds of automation will it aid? There are multiple ways to make this prediction and this blog talks about one such point of view. I would like to emphasise that this might not be the right answer but it is definitely an interesting one.]]></summary></entry><entry><title type="html">Ambigram</title><link href="https://soumik-kanad.github.io/blog/2023/ambigram/" rel="alternate" type="text/html" title="Ambigram"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ambigram</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ambigram/"><![CDATA[<p>Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.</p> <p>I realised that there aren’t any good places to put this on this website template without overcrowding it. So, I’m including them as a post.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-1400.webp"/> <img src="/assets/img/ambigram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-white-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-white-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-white-1400.webp"/> <img src="/assets/img/ambigram-white.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.]]></summary></entry><entry><title type="html">Are Transformers conceptually better than CNNs?</title><link href="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/" rel="alternate" type="text/html" title="Are Transformers conceptually better than CNNs?"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/"><![CDATA[<h2 id="an-inevitable-analogy">An inevitable analogy</h2> <p>Transformers evolved from attention-based RNNs when recurrence was removed from them. In layman’s terms, each layer of a Transformer tries to compare each atom of information (words for text and patches for images) with every other atom. While CNNs use the combination of convolution and pooling which only can compare spatially adjacent atoms while pooling the response from the convolution layers.</p> <p>I cannot miss the resemblance of these two architectures with sorting algorithms. In this analogy, a Transformer layer is the \(O(n^2)\) time complexity sorting algorithms (non-parallelised) like selection sort or bubble sort while CNNs are the \(O(n\log n)\) algorithms like merge sort<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. This also accounts for the higher complexity of Transformers<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Further, the comparisons at different receptive fields at different spatial resolutions in the CNNs are quite similar in structure to the recursive hierarchical comparison structure in mergesort. For example, in merge sort when we found the order in one level of recurrence we need not do comparisons among these sets of atoms again. Similarly, in CNNs pooling a patch leads to aggregation of information among all the atoms in a region and we need not compare them again amongst themselves.</p> <p>The only difference between sorting and these deep neural network architectures is that in the case of CNNs, pooling leads to loss of information due to which we cannot make sure that every atom gets compared to every other atom<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. Sorting assumes monotonicity among the atoms (a way to order the results of comparisons) which may not be the case for tasks that deep neural networks solve. In CNNs, only the pooled aggregations get “compared” with their neighbours while due to monotonicity aggregations of sorted subparts still retain all the information about their order. This problem does not occur in Transformers which uses a brute force way of comparison.</p> <p>I feel this may be the reason why <strong>Transformers are conceptually better than CNNs even though they may be way more inefficient</strong>. Ideally, the hierarchical nature of CNNs should extract all the essence from the input but in practice, <strong>it lacks the rigour that Transformers have and at the same time does not have the benefit of simplified input structure assumptions necessary for efficient sorting</strong>.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks to <a href="https://kampta.github.io/">Dr. Kamal Gupta</a> for their useful feedback to make this article more readable and explicit.</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>The \(\log n\) in the complexity comes from the number of layers required to have the possibility of comparison of all atoms in a CNN’s input. Some may say that one layer of Transformer is not a fair comparison for \(O(\log n)\) layers of convolution-pooling layers. But here I was trying to look at how much complexity is required to compare all the atoms (or at least have an illusion of it) which Transformers can do in a single layer. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>A multi-layer transformer will actually have a \(O(n^2 \log n)\) time complexity when \(O(\log n)\) layers are used. This is required so that higher-level concepts can be compared apart from low-level atoms. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://github.com/openai/CLIP/blob/main/clip/model.py#L58">Attention Pooling</a> seems to be a \(O(n)\) complexity compromise between standard pooling and a Transformer layer. If we were to use a fully functional Transformer layer instead for pooling then one could argue that why do even need convolutions anymore because <em>Attention is all you need</em> ! <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[An inevitable analogy]]></summary></entry></feed>