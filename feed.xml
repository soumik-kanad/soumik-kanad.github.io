<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://soumik-kanad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://soumik-kanad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-07-06T05:15:45+00:00</updated><id>https://soumik-kanad.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Ambigram</title><link href="https://soumik-kanad.github.io/blog/2023/ambigram/" rel="alternate" type="text/html" title="Ambigram"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ambigram</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ambigram/"><![CDATA[<p>Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.</p> <p>I realised that there aren’t any good places to put this on this website template without overcrowding it. So, I’m including them as a post.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-1400.webp"/> <img src="/assets/img/ambigram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-white-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-white-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-white-1400.webp"/> <img src="/assets/img/ambigram-white.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.]]></summary></entry><entry><title type="html">Are Transformers conceptually better than CNNs?</title><link href="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/" rel="alternate" type="text/html" title="Are Transformers conceptually better than CNNs?"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/"><![CDATA[<h2 id="an-inevitable-analogy">An inevitable analogy</h2> <p>Transformers evolved from attention-based RNNs when recurrence was removed from them. In layman’s terms, they try to compare each atom of information (words for text and patches for images) with every other atom. While CNNs use the combination of convolution and pooling which only can compare spatially adjacent atoms while pooling the response from the convolution layers.</p> <p>I cannot miss the resemblance of these two architectures with sorting algorithms. In this analogy, Transformers are the \(O(n^2)\) sorting algorithms like selection sort or bubble sort while CNNs are the \(O(n\log n)\) algorithms like merge sort. This also accounts for the slowness of Transformers. Further, the comparisons at different receptive fields at different spatial resolutions in the CNNs are quite similar in structure to the recursive hierarchical comparison structure in mergesort. For example, in merge sort when we found the order in one level of recurrence we need not do comparisons among these sets of atoms again. Similarly, in CNNs pooling a patch leads to aggregation of information among all the atoms in a region and we need not compare them again amongst themselves.</p> <p>The only difference between sorting and these deep neural network architectures is that in the case of CNNs, pooling leads to loss of information due to which we cannot make sure that every atom gets compared to every other atom. Sorting assumes monotonicity among the atoms (a way to order the results of comparisons) which may not be the case for tasks that deep neural networks solve. In CNNs, only the pooled aggregations get “compared” with their neighbours while due to monotonicity aggregations of sorted subparts still retain all the information about their order. This problem does not occur in Transformers which uses a brute force way of comparison.</p> <p>I feel this may be the reason why <strong>Transformers are conceptually better than CNNs even though they may be way more inefficient</strong>. Ideally, the hierarchical nature of CNNs should extract all the essence from the input but in practice, <strong>it lacks the rigour that Transformers have and at the same time does not have the benefit of simplified input structure assumptions necessary for efficient sorting</strong>.</p>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[An inevitable analogy]]></summary></entry></feed>