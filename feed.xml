<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://soumik-kanad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://soumik-kanad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-03T14:40:50+00:00</updated><id>https://soumik-kanad.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">What is the point?</title><link href="https://soumik-kanad.github.io/blog/2024/what-is-the-point/" rel="alternate" type="text/html" title="What is the point?"/><published>2024-02-27T00:00:00+00:00</published><updated>2024-02-27T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2024/what-is-the-point</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2024/what-is-the-point/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Why do I even bother to do research and submit to “1st Tier Conferences” when it is going to get rejected anyway? <strong>What is the point?</strong> My advisor says that the review process is very very noisy and we should not get disheartened; we should make improvements and resubmit again. He is probably right about the extreme noise because of the number of people submitting and the occasional experimentation in the review process. But I find the idea of “resubmitting again and again till your paper gets outdated and then someone pity-accepts the paper” inelegant and disheartening. I would have been fine with improving the work if there were actual pitfalls that needed to be corrected. But sigh! That is never the case.</p> <p>I am starting to find the process of review and rebuttal very emotionally draining as well as pointless. I’m not saying it does not work but it has not worked for me even once properly. By now I have gone through 4 cycles of reviews and rebuttals and every time it is just heart-breaking. There are only so many pieces that your heart can break into before it becomes irreparable. Besides, I realised that the immense amount of energy, effort, and time that goes into creating a work does not matter to a reviewer. If a reviewer has made up their mind in the initial review, it is highly improbable that they change their mind in the final review. Further, it does not even seem that it matters to the Area Chair/ Program Chair if the paper has any relevance.</p> <p>I am going to share the experience of the review of one of my papers. I think they are important to understand where I am coming from but feel free to skip them to the conclusion if you want.</p> <h2 id="example">Example</h2> <p>In this example, I talk about how a work of mine that got preposterous reviews. In this, we had an insight that (text-free) diffusion model features have emergent recognition powers, which makes diffusion process a self-supervised pretraining method. We first analysed the diversity in these features to be able to come up with methods to harness this knowledge in a meaningful manner. Then we also showed competitive results on multiple tasks to show its effectiveness.</p> <p>Some of the reviews were completely unrelated, eg. asking why we were text-free when the paper is in a self-supervised representation learning category, or why we didn’t compare/mention methods that use text; while others are unfair eg. disregarding a paper for not being SOTA, or questioning a hypothesis when we show empirical results; while some other are just plain wrong eg. misquoting papers, wrongly citing papers, saying that our method is the same as some completely different paper which models generation hence incremental, or disregarding the main heavy training stage of the SOTA competitor as an advantage we have. In regards to these, they go on to give comments which are very demoralizing as well as demeaning. And the PC final review was the final nail in the coffin being an utter concoction of balderdash.</p> <h3 id="version-1">version 1</h3> <p>I had a paper D1 which was majorly based on one primary insight (which I think was quite an exciting finding). The reviewers questioned (1) the motivation of the paper, (2) the fact that we don’t use “text”, (3) the performance gap, and (4) there being concurrent papers that explore one of all the aspects that we look at (obviously they do it differently). One of them asked to do 640 GPU hours of ablation (which we didn’t have the resources or time to do) and further goes on to say “I am not sure all reviewers will sufficiently raise their score for this paper to get through, but maybe this discussion is still useful to make a better paper.” (which is quite disheartening to get before the final review). Finally, the program chair rejected it quoting the “lack of thoroughness of this experimental study”, and the results being “mildly interesting” and “not [..] strong, impactful”. There was some constructive criticism as well which asked us to show results on more tasks and use the insight to build a method.</p> <h3 id="version-2">version 2</h3> <p>We took their advice for the next iteration of the paper, D2. In D2, D1 was merely a subpart of the analysis section. (1,2) We rewrote the introduction to handle the motivation and the “text-free” nature besides adding “text-free” in the title itself. (3) We introduced 2 new methods and showed their effectiveness on 5 tasks where were are now within +-2% of every method we compare against. (4) We also added a section in the related works talking about concurrent works that handle only one task using text-based models. So, we addressed all the flaws we had in the previous D1 and hence the paper should get in this time, right? Right?!</p> <p>You guessed it. The answer is NO!</p> <h4 id="initial-reviews--">Initial reviews -</h4> <p>The reviewers praise our writing, experiments, and analysis.</p> <ul> <li>R1 gives “borderline” based on “Related Works” being at the end and the fact that we didn’t spell out ViT-B! How is this relevant to any of the contents of the paper?</li> <li>R2 gives “borderline” and complains about SOTA and the methods being similar to a paper called UD. The only similarity UD has with one of our methods is that both use diffusion models and transformers. Now, the reviewer did not take the time out to see what UD was doing and what we were doing. UD was doing something that was completely different from what we were doing and that too very differently. Essentially, UD’s diffusion model was a transformer that was being trained to learn joint-distribution of text+image in their latent spaces while we used a pretrained “Convolutional” diffusion model’s features and gave this to a transformer head to do “Recognition”. If that was not enough, we also had a second method, which I think the reviewer fell asleep before reading. They, in fact, to prove a minor point of theirs misquoted a well-known paper we had as a reference.</li> <li>R3 gives “borderline” and complains about SOTA, missing reference of papers OD and VP, questions a hypothesis in the paper, and “text-free” nature of the paper. I was not aware of OD or VP, but both of these models again use text (leaking labels) and solve only one task while we show generalisation on multiple tasks. As for the hypothesis, they say stuff like “I am worried that the claim of … is not reliable and may be misleading to the research society” citing only one work. This is so disheartening when we experimentally show that this hypothesis works and had at least 5 citations in our related work which also have similar conclusions based on ablations in their tasks. By the way, this paper was submitted to “Self-Supervised Representation Learning” category, so I do not understand why they are questioning the “text-free” nature of the paper.</li> </ul> <h4 id="rebuttal--">Rebuttal -</h4> <ul> <li>We said that SOTA was not the focus, but showing that something trained for A works for B as well was our insight. But we have results within +-2% in all tasks that we show.</li> <li>Text-free: using text leaks downstream labels, manual/automated text captioner have their own bias and do not generalise to novel/out-of-distribution concepts. That is the whole point of self-supervised learning in the first place.</li> <li>About UD being different.</li> <li>We gave references for our hypothesis: [80, 49, 52, 67, 74]<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</li> <li>We gave results showing SOTA doesn’t beat us in a particular task.</li> <li>Some runtime numbers</li> </ul> <p>We pointed out to the AC that rejection based on SOTA is against the guidelines and that we were being questioned for “text-free” in “Self-Supervised Representation Learning” category. Also pointed out that R2 makes incorrect statements.</p> <h4 id="final-reviews--">Final reviews -</h4> <ul> <li>R1 ghosted us.</li> <li>R2 reduced rating to “weak reject”. They question SOTA and say that “it is not clear what conclusive additional information this work provides to the community.” They further question the computational overhead and are still stuck on UD being the same as ours. Finally, they question our hypothesis now saying that out of [80, 49, 52, 67, 74], [49, 52, 67, 74] are latent space diffusion models while ours is pixel space. How does that even prove any point?!!! Both latent-space, as well as pixel-space models, are going through the same diffusion process, and both are 2D spatial signals with channels; then how is the latent space so different? For a moment let’s assume that their point is valid. They slyly forget about [80] being pixel space. And what if I said that [52] was also a pixel space model and that R2 does not read things properly?</li> <li>R3 reduced the rating to “weak reject”. They say that OD, VP, and [49] already show what we showed in this work. Dear R3, you completely missed the point that this paper shows the property we highlight is true even without “text” showing that it is diffusion model’s property and not because of the enormous amount of text the rest are trained on. Besides, they each show one task at a time while we show multiple. We had pointed out that the SOTA model requires 3 cumbersome steps which they did not agree saying that step 1 of that is readily available similar to ours while step 3 is similar to ours; completely forgetting step 2 which requires training on 32 V100s for 4 days. Finally, they rejected based on SOTA.</li> <li>PC: used these phrases “below CVPR standards”, “unconvincing experiments and performance improvements”, “unjustified design choices”, and “incremental contributions”, “needs substantial rewriting”. I don’t understand if the AC/PC even read the paper properly because I don’t see them summarising the paper at all. It just seems that they are basing their arguments on the reviews completely without even reading the paper or rebuttal. Even if I were to base the final decision on the reviewers, who in their final review did have “performance improvements” (which is against the guideline) and “incremental contributions” (which I don’t think is correct based on my comments above); none of them had problems with “unconvincing experiments”, “unjustified design choices”, or “needs substantial rewriting”. Where did they even get these three criteria? <ul> <li><strong>“Unconvincing experiments”</strong>: R1 R2 put extensive experiments in varied tasks as our strength.</li> <li><strong>“Unjustified design choices”</strong>: R2 praised us saying “Fairly easy to implement, has potential to have a significant impact” and R3 said that our analysis was “interesting”, “novel” and “inspiring” on which our methods were based. Maybe the PC was talking about the hypothesis here? But we have extensive studies and references backing it.</li> <li><strong>“needs substantial rewriting”</strong> except R1 having an issue with the related work being at the end, both R2 and R3 praised the clarity and flow of the paper. R2 had a few minor updates which we were ready to make.</li> </ul> </li> </ul> <h2 id="discussion-and-conclusion">Discussion and Conclusion</h2> <p>I don’t think I have a conclusion because I am both confused and disappointed. How do you have faith in such a system where scientific evidence is not enough to convince the community to accept a work? Here the paper will get accepted only if your views align with those of the reviewers (who apparently can represent the whole community). Here the reviewers already make up their minds based on their beliefs and will not change it even with evidence bolstering your point. Here new insights are useless if you do not beat the SOTA. And if you do beat the SOTA your work is not novel enough. I can no longer even have faith in the AC/PC system as well because that also seems noisy. The chairs seem to not properly read the paper or the rebuttals to base their final judgment. I appreciate the reviewers, ACs, PCs to take out their time to undergo the review process but I am losing all my faith in the outcome and effectiveness of this system. Anyway, people are using LLMs to write their reviews, might as well remove the middle-“MAN”. Scaling may solve video generation but it seems to fail miserably in peer-review.</p> <p>When a very similar study was done on a very small scale with results on CiFAR, MNIST and consequently got performance comparable to standard methods, even though not SOTA, it ended up getting an “Oral paper”. But us showing that this insight does not hold in larger datasets like ImageNet out of the box was “mildly interesting” and proposing methods to make it comparable to standard methods was “unconvincing [..] performance improvements”, “unjustified design choices”, and “incremental contributions”.</p> <p>I don’t even understand why they gave this paper borderline in the first place if they didn’t want to accept it. Were they expecting that we would make it SOTA in the rebuttal period or did I say things in the rebuttal that they didn’t agree with? The PC came up with new unfair reasons to reject the paper without proper justification.</p> <p>One conclusion could be that the system is extremely flawed. But the flip side would be that I’m not good enough for this field. My ideas are boring, incremental, and unacceptable for this fast-moving community. All ideas anyway get outdated and are declared useless because of concurrent works in a similar domain (even though they can never be the exact same). Anyway, there is no breathing space, the reviews/rebuttals further choke all the enthusiasm out of you. To catch up, currently, I put all my hours in the day toward research. And still, I lag in the amount of work that I produce and out of that even less gets accepted.</p> <p>In the past three years of my PhD I have dedicated most of my time to work, leading to the loss of my hobbies. I knew that PhD would be difficult but it is also draining me from inside. Yesterday, when I asked what was the point of any of this it was going to get rejected anyway. My advisor said that you should do it because it makes you happy. And doing research makes me immensely happy but at the same time, these unjustified rejections make me extremely sad and I’m starting to question if the latter is more. So,back to the question – <strong>what is the point</strong>?</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>These references are from the original paper. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Introduction Why do I even bother to do research and submit to “1st Tier Conferences” when it is going to get rejected anyway? What is the point? My advisor says that the review process is very very noisy and we should not get disheartened; we should make improvements and resubmit again. He is probably right about the extreme noise because of the number of people submitting and the occasional experimentation in the review process. But I find the idea of “resubmitting again and again till your paper gets outdated and then someone pity-accepts the paper” inelegant and disheartening. I would have been fine with improving the work if there were actual pitfalls that needed to be corrected. But sigh! That is never the case.]]></summary></entry><entry><title type="html">Claiming something was already discovered in ancient India, a case study.</title><link href="https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws/" rel="alternate" type="text/html" title="Claiming something was already discovered in ancient India, a case study."/><published>2023-12-06T00:00:00+00:00</published><updated>2023-12-06T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ancient-india-and-newtons-laws/"><![CDATA[<p>A lot of Indians love to claim that scientific things were already discovered/invented in ancient India. Although, this is not always incorrect (you can find Panini’s name in <a href="https://en.wikipedia.org/wiki/List_of_pioneers_in_computer_science">the list of pioneers in computer science</a> ), most of these claims are unsubstantiated. Recently, I came across a youtube video claiming Netwon’s Laws of Motion were alreay discovered by <a href="https://en.wikipedia.org/wiki/Ka%E1%B9%87%C4%81da_(philosopher)">Kanada</a>, an ancient Indian plilosopher who is known for his atomist approach of matter (Kana). A big problem with such videos is that they don’t do their research very well and just publish anything that resonates with their ideology.</p> <h2 id="tldr">tl;dr</h2> <p>Please properly check your sources before claiming that “it was already discovered in ancient India”. It is possible that it was but please don’t publish it till you check it, otherwise it just becomes misinformation.</p> <h2 id="validity-of-the-video">Validity of the Video</h2> <iframe width="315" height="560" src="https://www.youtube.com/embed/dCMPy5DHoMs?si=vhJdu9lS-1o8UezZ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> <p>A short video titled <a href="https://youtube.com/shorts/dCMPy5DHoMs?si=vhJdu9lS-1o8UezZ">Is NEWTON Really GREAT? | This will Shock you</a> was published on Youtube. I got intrigued because I share my nickname with Rishi Kanada, a name I got from my late maternal grandfather. This video cites a 2020 paper named <a href="https://www.ajer.org/papers/Vol-9-issue-7/K09078792.pdf">Origin of Laws of Motion (Newton’s Law): An Introspective Study</a> which was written by a 2<sup>nd</sup> year bachelor’s student without any supervision and was publised in a <a href="https://www.ajer.org">questionable journal</a><d-footnote><a href="https://flakyj.blogspot.com/2018/12/american-journal-of-engineering.html">https://flakyj.blogspot.com/2018/12/american-journal-of-engineering.html</a></d-footnote><d-footnote><a href="https://twitter.com/fake_journals/status/1057638078888591360?lang=en">https://twitter.com/fake_journals/status/1057638078888591360?lang=en</a></d-footnote>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/paper_screenshot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/paper_screenshot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/paper_screenshot-1400.webp"/> <img src="/assets/img/blog/paper_screenshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For a second, even if I were to stop questioning this journal and the credential of the author, and go into the paper itself, it still has problems. The sutras (principles) being talked about in the paper don’t even exist in <a href="https://indianculture.gov.in/rarebooks/vaisesika-sutras-kanada-commentary-sankara-misra-and-extracts-gloss-jayanarayana">The Vaisheshika Sutras of Kanada</a>. Yes, I checked and none of the sutras start with the word “Vegah” (वेगः meaning impetus). After some reverse engineering I found a <a href="https://www.booksfact.com/science/ancient-science/vaishesika-sutras-by-kanada-describe-laws-of-motion-concept-of-atom.html">2013 webpage with identical sutras</a> and a <a href="https://www.quora.com/Did-Newton-take-three-laws-of-motion-from-Ancient-Indian-scriptures">2018 quora question</a> (which probably also picked these up from the former) which have these sutras.</p> <p><b>This means that someone made these up and others are propagating this misinformation to the extent that someone published it in a questionable journal and someone else used that paper to bolster their claim in a video. </b> There is already so much misinformation and “already discovered in ancient India” claims don’t help stuff that were actually discovered in ancient India. People have started considering “already discovered in ancient India” as a joke.</p> <h2 id="were-newtons-laws-of-motion-discovered-by-kanada-first">Were Newton’s Laws of Motion discovered by Kanada first?</h2> <p>In the previous section I have given evidence as to why the sources of the video linked is fabricated. But that doesn’t prove the claim wrong and one might wonder if the claim if actually correct. The answer to that is yes, and no. I did come across an insightful blog by <a href="https://en.wikipedia.org/wiki/Subhash_Kak">Dr. Subhash Kak</a> titled <a href="https://subhashkak.medium.com/ka%E1%B9%87%C4%81da-the-great-physicist-and-sage-of-antiquity-5a2abd79b6f1">“Kaṇāda, Great Physicist and Sage of Antiquity”</a> which does point out to sutras (which actually exist in Vaisheshika Sutras, I checked) which might have the essence of being predecessor ideas to more concrete and explicit versions of Newton’s Laws of Motion. Check out yourself.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog_screenshot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog_screenshot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog_screenshot-1400.webp"/> <img src="/assets/img/blog/blog_screenshot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I also found a <a href="https://www.quora.com/Did-Newton-take-three-laws-of-motion-from-Ancient-Indian-scriptures/answer/Abdul-Azarudeen-1?comment_id=182815984&amp;comment_type=2">comment in the quora question’s answers</a> which did point to exact sutras corresponding to each of the laws:</p> <h3 id="1st-law">1st law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/1st_law-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/1st_law-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/1st_law-1400.webp"/> <img src="/assets/img/blog/1st_law.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="2nd-law">2nd law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2nd_law1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2nd_law1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2nd_law1-1400.webp"/> <img src="/assets/img/blog/2nd_law1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2nd_law2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2nd_law2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2nd_law2-1400.webp"/> <img src="/assets/img/blog/2nd_law2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="3rd-law">3rd law</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/3rd_law-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/3rd_law-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/3rd_law-1400.webp"/> <img src="/assets/img/blog/3rd_law.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="conclusion">Conclusion</h3> <p>Even though not exactly the same, these do point to the fact the ancient Indian philosophers might have been thinking in the right direction. This is probably the difference between philosophy and science (I will write a blog about this in the future). And I appreciate the efforts of Rishi Kanada for this. <b>It is especially very surprising for me that he nailed the 3rd law before the Common Era (BCE)</b>. Which is very cool!</p> <p>If you are interested here are 2 different translations/interpretations of “The Vaisheshika Sutras of Kanada”:</p> <ol> <li><a href="https://indianculture.gov.in/rarebooks/vaisesika-sutras-kanada-commentary-sankara-misra-and-extracts-gloss-jayanarayana">The Vaisesika Sutras of Kaṇada: With the Commentary of Sankara Misra and Extracts From the Gloss of Jayanarayaṇa </a></li> <li><a href="https://www.omraizada.com/home/scriptures/Vaisheshika%20Sutra-of-Kanada-Debashish-Chakrabarty.pdf">Vaisesika Sutra of Kanada, Translated by Debasish Chakravarty</a></li> </ol>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[A lot of Indians love to claim that scientific things were already discovered/invented in ancient India. Although, this is not always incorrect (you can find Panini’s name in the list of pioneers in computer science ), most of these claims are unsubstantiated. Recently, I came across a youtube video claiming Netwon’s Laws of Motion were alreay discovered by Kanada, an ancient Indian plilosopher who is known for his atomist approach of matter (Kana). A big problem with such videos is that they don’t do their research very well and just publish anything that resonates with their ideology.]]></summary></entry><entry><title type="html">AI vs Jobs?</title><link href="https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements/" rel="alternate" type="text/html" title="AI vs Jobs?"/><published>2023-07-12T00:00:00+00:00</published><updated>2023-07-12T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ai-and-job-replacements/"><![CDATA[<p>Have you ever wondered which kinds of jobs are easier to automate? For example, with the advent of the Industrial Revolution, electro-mechanical machines could automate a lot of repetitive (“non-intelligent”) physical labour. Similarly, computers have automated a lot of computations that were done by hand in the olden days. Now, as we see how powerful AI can be, the next question is which kinds of automation will it aid? There are multiple ways to make this prediction and this blog talks about one such point of view. I would like to emphasise that this might not be the right answer but it is definitely an interesting one.</p> <h2 id="tldr">tl;dr</h2> <p>It took hundreds of times more time for locomotion to evolve than human level intelligence. Is this why it seems that creative jobs are going to be replaced by AI before physical labour jobs as the former is an easier problem to solve?</p> <h2 id="blue-collar-vs-white-collar">Blue collar vs white collar</h2> <p>In a simplified classification of jobs, one could categorise them into three classes - physical labour jobs, mental labour jobs, and creative jobs. There could be other categories or jobs that might not fit these categories but I think these are quite prevalent. Physical labour jobs include jobs like construction jobs, plumbing, electrical jobs, and other physical maintenance jobs while mental labour jobs include other administrative desk jobs which do need a skill set at times but may not need creativity as such like clerical jobs, annotators, programmers, customer service, etc. Finally, creative jobs include jobs like music, art, research, etc. where something new is being created.</p> <p>Scientists in the previous century thought that if we were successful in creating artificial intelligence (AI), it would take jobs in the same order of categories as I list them above<d-footnote>Need citation.</d-footnote>. I think this was thought because people put these categories of jobs in a hierarchy of prestige. This comes from the notion that a lot of people can work in a factory while not many can write poetry or a paper. But as we move closer to the middle of the 21<sup>st</sup> century, it seems that the opposite order may be more probable. Today we see that just with prompts one can create beautiful art, music, or poetry but we are far away from perfectly working autonomous cars or other forms of robotics.</p> <p>Have you ever wondered how we reached this point where more people can do physical labour than creative thinking? One interesting analogy to see why that may be the case lies in the evolution of human intelligence vs animal locomotion. It is noteworthy to see that human intelligence evolved only recently, for example, humans started using tools only 3.5 million years (Pliocene epoch <d-footnote><a href="https://en.wikipedia.org/wiki/Evolution_of_human_intelligence">https://en.wikipedia.org/wiki/Evolution_of_human_intelligence</a></d-footnote>) ago. On the other hand locomotion in animals date back to at least 560 million years <d-footnote><a href="https://www.scientificamerican.com/article/the-rise-of-the-first-animals">https://www.scientificamerican.com/article/the-rise-of-the-first-animals/</a></d-footnote>. This shows how much evolution was required to perfect (or rather just to make functional) our physical movement (with which also comes planning) while only a fraction of that time was invested in evolving human intelligence.</p> <p>This may not be perfect evidence but does <strong>point towards the possibility of creativity being an easier problem to solve than physical labour</strong>. Even though this is not proof of any sort but it is a powerful idea and worth thinking about. Another reason that might have caused creatively intelligent tasks solved before more physically skilled tasks may be because of the abundance of data in the creative world due to the internet compared to the physical labour world. But these are all speculations and only time will tell which was the actual reason.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>I don’t think this is my original thought. I came across this in a video that had a Youtuber called <a href="https://www.youtube.com/@VarunMayya">Varun Mayya</a>. I tried finding it but could find it given the abundance of these short-form videos these days.</p> ]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Have you ever wondered which kinds of jobs are easier to automate? For example, with the advent of the Industrial Revolution, electro-mechanical machines could automate a lot of repetitive (“non-intelligent”) physical labour. Similarly, computers have automated a lot of computations that were done by hand in the olden days. Now, as we see how powerful AI can be, the next question is which kinds of automation will it aid? There are multiple ways to make this prediction and this blog talks about one such point of view. I would like to emphasise that this might not be the right answer but it is definitely an interesting one.]]></summary></entry><entry><title type="html">Ambigram</title><link href="https://soumik-kanad.github.io/blog/2023/ambigram/" rel="alternate" type="text/html" title="Ambigram"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/ambigram</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/ambigram/"><![CDATA[<p>Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.</p> <p>I realised that there aren’t any good places to put this on this website template without overcrowding it. So, I’m including them as a post.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-1400.webp"/> <img src="/assets/img/ambigram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ambigram-white-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ambigram-white-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ambigram-white-1400.webp"/> <img src="/assets/img/ambigram-white.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[Ambigrams are text/pictures which when rotated by 180 degrees, still represent something meaningful. If you have read ‘Angels and Demons’ by Dan Brown you already know about it. I had created these because the online generators failed for my first name.]]></summary></entry><entry><title type="html">Are Transformers conceptually better than CNNs?</title><link href="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/" rel="alternate" type="text/html" title="Are Transformers conceptually better than CNNs?"/><published>2023-07-05T00:00:00+00:00</published><updated>2023-07-05T00:00:00+00:00</updated><id>https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn</id><content type="html" xml:base="https://soumik-kanad.github.io/blog/2023/transformer-vs-cnn/"><![CDATA[<h2 id="an-inevitable-analogy">An inevitable analogy</h2> <p>Transformers evolved from attention-based RNNs when recurrence was removed from them. In layman’s terms, each layer of a Transformer tries to compare each atom of information (words for text and patches for images) with every other atom. While CNNs use the combination of convolution and pooling which only can compare spatially adjacent atoms while pooling the response from the convolution layers.</p> <p>I cannot miss the resemblance of these two architectures with sorting algorithms. In this analogy, a Transformer layer is the \(O(n^2)\) time complexity sorting algorithms (non-parallelised) like selection sort or bubble sort while CNNs are the \(O(n\log n)\) algorithms like merge sort<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. This also accounts for the higher complexity of Transformers<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Further, the comparisons at different receptive fields at different spatial resolutions in the CNNs are quite similar in structure to the recursive hierarchical comparison structure in mergesort. For example, in merge sort when we found the order in one level of recurrence we need not do comparisons among these sets of atoms again. Similarly, in CNNs pooling a patch leads to aggregation of information among all the atoms in a region and we need not compare them again amongst themselves.</p> <p>The only difference between sorting and these deep neural network architectures is that in the case of CNNs, pooling leads to loss of information due to which we cannot make sure that every atom gets compared to every other atom<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. Sorting assumes monotonicity among the atoms (a way to order the results of comparisons) which may not be the case for tasks that deep neural networks solve. In CNNs, only the pooled aggregations get “compared” with their neighbours while due to monotonicity aggregations of sorted subparts still retain all the information about their order. This problem does not occur in Transformers which uses a brute force way of comparison.</p> <p>I feel this may be the reason why <strong>Transformers are conceptually better than CNNs even though they may be way more inefficient</strong>. Ideally, the hierarchical nature of CNNs should extract all the essence from the input but in practice, <strong>it lacks the rigour that Transformers have and at the same time does not have the benefit of simplified input structure assumptions necessary for efficient sorting</strong>.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks to <a href="https://kampta.github.io/">Dr. Kamal Gupta</a> for their useful feedback to make this article more readable and explicit.</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>The \(\log n\) in the complexity comes from the number of layers required to have the possibility of comparison of all atoms in a CNN’s input. Some may say that one layer of Transformer is not a fair comparison for \(O(\log n)\) layers of convolution-pooling layers. But here I was trying to look at how much complexity is required to compare all the atoms (or at least have an illusion of it) which Transformers can do in a single layer. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>A multi-layer transformer will actually have a \(O(n^2 \log n)\) time complexity when \(O(\log n)\) layers are used. This is required so that higher-level concepts can be compared apart from low-level atoms. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://github.com/openai/CLIP/blob/main/clip/model.py#L58">Attention Pooling</a> seems to be a \(O(n)\) complexity compromise between standard pooling and a Transformer layer. If we were to use a fully functional Transformer layer instead for pooling then one could argue that why do even need convolutions anymore because <em>Attention is all you need</em> ! <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Soumik Mukhopadhyay</name></author><summary type="html"><![CDATA[An inevitable analogy]]></summary></entry></feed>